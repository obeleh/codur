# Codur Configuration
# Main configuration file for the Codur autonomous coding agent

# MCP Servers - Define external services that can be called
mcp_servers:
  sheets:
    command: "/Users/sjuul/workspace/mcp-servers/.venv/bin/python"
    args: ["/Users/sjuul/workspace/mcp-servers/sheets/mcp_server.py"]
    cwd: "/Users/sjuul/workspace/mcp-servers/sheets"
    env: {}

  linkedin:
    command: "/Users/sjuul/workspace/mcp-servers/.venv/bin/python"
    args: ["/Users/sjuul/workspace/mcp-servers/linked-in/mcp_server.py"]
    cwd: "/Users/sjuul/workspace/mcp-servers/linked-in"
    env: {}

# Agent Settings - Configure which agents are available and their routing
# Not to be edited by Agents
agents:
  preferences:
    default_agent: "agent:groq-qwen3-32b"
    fallback_model: "groq-70b"

    # Routing rules: which agent to use for which task type
    routing:
      simple: "llm:groq-qwen3-32b"   # Simple code generation (Groq)
      complex: "llm:groq-qwen3-32b"  # Complex refactoring (OpenAI)
      multifile: "agent:claude_code" # Multi-file changes (Claude Code)
      reasoning: "agent:groq-qwen3-32b-reasoning"      # Complex reasoning (Groq)
      sheets: "sheets"               # Spreadsheet operations
      jobs: "linkedin"               # Job searching

    # Fallback order if primary agent fails
    fallback_order:
      - ollama
      - claude_code
      - codex

  # Agent-specific configurations
  configs:
    groq-qwen3-32b:
      name: "groq"
      type: "llm"
      enabled: true
      config:
        model: "qwen/qwen3-32b"
        system_prompt: "You are a helpful coding assistant. Provide clear and concise code snippets."

    groq-qwen3-32b-reasoning:
      name: "groq"
      type: "llm"
      enabled: true
      config:
        model: "qwen/qwen3-32b"
        system_prompt: "You are a helpful coding assistant. Be thorough and detailed in your reasoning steps when solving complex coding problems."
        reasoning_effort: "high"

    groq-llama3-70b:
      name: "groq"
      type: "llm"
      enabled: true
      config:
        model: "llama-3.1-70b-versatile"

    ollama:
      name: "ollama"
      type: "llm"
      enabled: true
      config:
        model: "ministral-3:14b"

    codex:
      name: "codex"
      type: "tool"
      enabled: true
      config:
        command: "codex"
        model: "gpt-5-codex"
        reasoning_effort: "medium"

    claude_code:
      name: "claude_code"
      type: "tool"
      enabled: true
      config:
        command: "claude"
        model: "sonnet"
        max_tokens: 8000

    sheets:
      name: "sheets"
      type: "mcp"
      enabled: true
      config:
        mcp_server: "sheets"

    linkedin:
      name: "linkedin"
      type: "mcp"
      enabled: true
      config:
        mcp_server: "linkedin"

  profiles:
    ollama-ministral:
      name: "ollama"
      type: "llm"
      enabled: true
      config:
        model: "ministral-3:14b"
        system_prompt: "You are a concise coding assistant. Summarize code behavior and structure clearly. Avoid vague placeholders."

# LLM Settings - Main orchestrator LLM configuration
llm:
  default_profile: groq-openai-oss-120b
  default_temperature: 0.7
  planning_temperature: 0.3   # Lower temperature for more deterministic JSON output in planning
  generation_temperature: 0.5 # Normal temperature for code generation

  profiles:
    groq-openai-oss-120b:
      provider: "groq"
      model: "openai/gpt-oss-120b"
    groq-openai-oss-20b:
      provider: "groq"
      model: "openai/gpt-oss-20b"
    gemini-3-flash-preview:
      provider: "ollama"
      model: gemini-3-flash-preview
    groq-70b:
      provider: "groq"
      model: "llama-3.3-70b-versatile"
    groq-qwen3-32b:
      provider: "groq"
      model: "qwen/qwen3-32b"
    openai-gpt-5:
      provider: "openai"
      model: "gpt-5"
    openai-gpt-5-mini:
      provider: "openai"
      model: "gpt-5-mini"
    ollama-ministral:
      provider: "ollama"
      model: "ministral-3:14b"

providers:
  anthropic:
    api_key_env: "ANTHROPIC_API_KEY"
  openai:
    api_key_env: "OPENAI_API_KEY"
  groq:
    api_key_env: "GROQ_API_KEY"
  ollama:
    base_url: "http://localhost:11434"
    max_model_size_gb: 16

# Runtime Settings - Control agent behavior and async execution
runtime:
  max_iterations: 10
  # Global timeout for a single Codur run (seconds). Set to null to disable.
  max_runtime_s: null
  verbose: true
  allow_outside_workspace: true
  # Workspace root used for resolving relative paths (auto-set on startup if omitted).
  workspace_root: null
  # This allows tool detection outside LLM calls (RECOMMENDED: keeps it reliable)
  detect_tool_calls_from_text: true
  # Planner fallback profiles if primary LLM fails
  planner_fallback_profiles:
    - groq-70b
    - ollama-ministral

  # Async execution settings
  async:
    max_concurrent_agents: 3      # Max agents running in parallel
    event_buffer_size: 200         # Size of event queue
    stream_events: true            # Stream events as they happen
    user_input_poll_ms: 100        # How often to check for user input
    tool_timeout_s: 600            # Timeout for tool execution (10 min)
    cancel_grace_s: 5              # Grace period for cancellation

# Planning Settings - Control planning behavior
planning:
  debug_truncate_short: 300        # Reduce debug output length
  debug_truncate_long: 800         # Reduce error message length
  max_retry_attempts: 2            # Fewer retries for faster failure
  retry_initial_delay: 0.3         # Faster initial retry
  retry_backoff_factor: 1.5        # Less backoff between retries

# Agent Execution Settings - Control how agents are executed
agent_execution:
  default_cli_timeout: 300         # 5 minutes instead of 10
  claude_code_max_tokens: 8000     # Max tokens for Claude Code

# Model Agent Instructions - Counter model biases with custom instructions
# These instructions are injected as system messages before LLM invocations
# matching the agent pattern in the invoked_by parameter
model_agent_instructions:
  # Apply to all LLM invocations (planning, coding, explaining, etc.)
  # - agent: "all"
  #   instruction: "Be concise and avoid unnecessary explanations unless specifically asked."
  # Apply only to coding-related LLM invocations (coding.primary, coding.retry, etc.)
  - agent: "coding"
    instruction: "Do not suddenly inject reading from stdin. Only read from stdin if explicitly requested by the user."
  # Apply only to planning LLM invocations (planning.llm_plan, planning.retry_json, etc.)
  # - agent: "planning"
  #   instruction: "Prefer delegating to local tools over external API calls when possible."

# Tool settings
tools:
  # Allow git write operations like staging or commits.
  allow_git_write: true
